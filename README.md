# vk_internship_test_bot

Id бота - @viilyk_knowledge_bot

Бот принимает файлы в форматах PDF, DOCX, TXT и MD и возвращает пользователю суммаризацию документа и набор ключевых слов. Информация о каждой операции сохраняется в Google Sheets.

Для разработки бота использовался n8n.

[__workflow (json)__](vk_internship_bot_workflow.json)

Ссылка на таблицу - https://docs.google.com/spreadsheets/d/1K3eGfBHNwwXk644BOCyqLDlKXi4Vj4cNoCKRb51IJk4/edit?gid=0#gid=0

### Особенности:
-OCR подключён, но используется только для PDF-файлов, в которых нет машиночитаемого текста. Действуют лимиты на размер (до 1 MB) и страницы (не более 3). Для OCR доступен только русский язык.

## Использованные инструменты:
- Для загрузки и парсинга сырого текста из PDF, TXT и MD использовались ноды типа Extract from File. Данный подход прост и достаточен для поставленных задач.
- Для обработки DOCX применялась связка Google Drive + Google Docs: файлы загружаются на диск и конвертируются в TXT, так как в n8n отсутствуют встроенные ноды для чтения DOCX-файлов. Данное API было выбрано из-за бесплатности, надёжности и относительной простоты. Хотя оно имеет свои минусы: лишние шаги, долгая обработка и зависимость от стороннего сервиса.
- Для суммаризации и выделения ключевых слов использовалась модель Qwen / Qwen2.5-7B-Instruct через Hugging Face API. Решение было сделано на основе того, что в Hugging Face есть бесплатные лимиты без привязки карты, модель хорошо работает для русского языка, выдает приличное качество и достаточно легко интегрируется.
- В качестве OCR-инструмента выбран OCR.Space. Выбор пал на этот инструмент так как у него есть бесплатный лимит, он просто интегрируется. Но получаемое качество далеко от идеала. Модель плохо работает на сложных сканах, а также плохо справляется с мультиязычностью. OCR используется только в случае если текст отсутствует для экономии ресурсов и избежания снижения качества и трудностей с агрегацией.

## Трудности:
- Наиболее сложным этапом оказалась отправка запроса к LLM. Потребовалось согласовывать выводы из разных веток workflow, а также выполнять экранирование и замену символов в тексте для корректной передачи через JSON.
- Отдельной проблемой стал выбор инструментов: хотелось избежать сложностей с оплатой иностранных сервисов и не выходить за рамки работы над workflow (что можно было сделать наприемер для обработки DOCX и для подключения Tesseract)

## Варианты улучшений:
- Бот не распознаёт изображения и сканы в документах, содержащих как нативный текст, так и графику. Для решения этой проблемы можно использовать LlamaParse, который оптимизирован для таких случаев. Он использует OCR выборочно и сохраняет структуру.
- Реализованный бот не извлекает текст с изображений внутри DOCX-файлов. OCR.Space не поддерживает DOCX. Это можно исправить, добавив логику извлечения изображений из файла и их передачи в OCR-модель или заменой модели.
- Использование более мощных моделей для повышения качества обработки и ослабления ограничений по размеру и языкам. В текущей реализации особенно заметно снижение качества после OCR. В качестве основного текстового инструмента можно рассмотреть GPT или Deepseek, а для OCR - Mistral OCR, LlamaParse.
- Добавить разбиение текста на фрагменты перед отправкой в LLM, чтобы обрабатывать длинные документы частями. Это позволит снизить нагрузку на модель и обработать текст полностью (сейчас тексты обрезаются по символам)
- В текущем решении таблицы преобразуются в линейный текст или извлекаются через OCR, из-за чего теряется их структура. Для базовой суммаризации это некритично, но при работе с документами, где важны числовые данные и сравнения, качество результата снижается. Поэтому улучшение парсинга таблиц имеет смысл для повышения точности интерпретации содержания.
- В MD-файлах могут присутствовать изображения, вставленные через ссылки - их также можно извлекать и обрабатывать, хотя в большинстве случаев это не должно быть критично.
- Реализовать собственный парсер DOCX внутри Docker-контейнера для снижения зависимости от сторонних сервисов. 
- Добавить в workflow остановку при превышении лимита размера PDF-файла для OCR и вывод сообщений. 
- Добавить боту больше гибкости. Например предоставить пользователю возможности: выбор диапозона страниц для обработки; исключение страниц для обработки; настройка размера суммаризации; выбор языка вывода; добавление дополнительных данных для контекста (словарь, предыдущие документы).
- Добавить функцию поиска похожих документов в базе.

## Скрины workflow
*Общий вид:*

<img width="1174" height="409" alt="Снимок экрана 2025-12-15 в 07 17 47" src="https://github.com/user-attachments/assets/3765c878-0321-4ed8-ae0a-05ae36ccc4db" />


*Обработка сообщений:*

<img width="738" height="326" alt="Снимок экрана 2025-12-15 в 07 22 16" src="https://github.com/user-attachments/assets/97658ca2-d637-41b3-8f6f-99e49eed60e7" />

В функции определяется ответное сообщение в зависимости от введенного. Рассматриваются случаи для ввода команд /start, /help и для произвольного сообщения не содержащего файл.


*DOCX Parsing:*

<img width="558" height="198" alt="Снимок экрана 2025-12-15 в 07 25 12" src="https://github.com/user-attachments/assets/c75a44d6-2ccb-41b0-b78d-b9f154e6a6dc" />

В HTTP Request происходит отправка запроса для конвертации docx в txt. Edit Fields - переименование поля с текстом для однотипного выхода ветвей расширений.


*PDF-парсинг:*

<img width="492" height="191" alt="Снимок экрана 2025-12-15 в 07 26 15" src="https://github.com/user-attachments/assets/8c7ec704-183a-42df-a8c9-11f337671ac5" />Ш

В IF - проверка текста на пустоту. В HTTP Request 2 - запрос к OCR.Space. В Code for Concat Input - конкатенация постраничного вывода модели.


*TXT/MD-Parsing:*

<img width="492" height="191" alt="Снимок экрана 2025-12-15 в 07 31 17" src="https://github.com/user-attachments/assets/dbafd938-a81f-465e-963e-e3d52dab9224" />

Edit Fields1 - переименование поля с текстом для однотипного выхода ветвей расширений.


*Запрос к LLM и запись в Google Sheets:*

<img width="492" height="198" alt="Снимок экрана 2025-12-15 в 07 33 20" src="https://github.com/user-attachments/assets/0ded41c9-69ae-411f-b389-6f60e6e83164" />

Code for Text - экранирование и замена символов в тексте для последующей подстановки в JSON и обрезка по символам. В HTTP Request1 запрос к LLM с промтом. В Code for Parse Result - простой парсинг выхода модели. Далее идет запись в Google Sheets и отправка результата пользователю.
