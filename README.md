# vk_internship_test_bot

Id бота - @viilyk_knowledge_bot

Бот принимает файлы в форматах PDF, DOCX, TXT и MD и возвращает пользователю суммаризацию документа и набор ключевых слов. Информация о каждой операции сохраняется в Google Sheets.

Для разработки бота использовался n8n.

Ссылка на таблицу - https://docs.google.com/spreadsheets/d/1K3eGfBHNwwXk644BOCyqLDlKXi4Vj4cNoCKRb51IJk4/edit?gid=0#gid=0

### Особенности:
-OCR подключён, но используется только для PDF-файлов, в которых нет машиночитаемого текста. Действуют лимиты на размер (до 1 MB) и страницы (не более 3). Для OCR доступен только русский язык.

## Использованные инструменты:
- Для загрузки и парсинга сырого текста из PDF, TXT и MD использовались ноды типа Extract from File. Данный подход прост и достаточен для поставленных задач.
- Для обработки DOCX применялась связка Google Drive + Google Docs: файлы загружаются на диск и конвертируются в TXT, так как в n8n отсутствуют встроенные ноды для чтения DOCX-файлов. Данное API было выбрано из-за бесплатности, надёжности и относительной простоты. Хотя оно имеет свои минусы: лишние шаги, долгая обработка и зависимость от стороннего сервиса.
- Для суммаризации и выделения ключевых слов использовалась модель Qwen / Qwen2.5-7B-Instruct через Hugging Face API. Решение было сделано на основе того, что в Hugging Face есть бесплатные лимиты без привязки карты, модель хорошо работает для русского языка, выдает приличное качество и достаточно легко интегрируется.
- В качестве OCR-инструмента выбран OCR.Space. Выбор пал на этот инструмент так как у него есть бесплатный лимит, он просто интегрируется. Но получаемое качество далеко от идеала. Модель плохо работает на сложных сканах, а также плохо справляется с мультиязычностью. OCR используется только в случае если текст отсутствует для экономии ресурсов и избежания снижения качества и трудностей с агрегацией.

## Трудности:
- Наиболее сложным этапом оказалась отправка запроса к LLM. Потребовалось согласовывать выводы из разных веток workflow, а также выполнять экранирование и замену символов в тексте для корректной передачи через JSON.
- Отдельной проблемой стал выбор инструментов: хотелось избежать сложностей с оплатой иностранных сервисов и не выходить за рамки работы над workflow (что можно было сделать наприемер для обработки DOCX и для подключения Tesseract)

## Варианты улучшений:
- Бот не распознаёт изображения и сканы в документах, содержащих как нативный текст, так и графику. Для решения этой проблемы можно использовать LlamaParse, который оптимизирован для таких случаев. Он использует OCR выборочно и сохраняет структуру.
- Реализованный бот не извлекает текст с изображений внутри DOCX-файлов. OCR.Space не поддерживает DOCX. Это можно исправить, добавив логику извлечения изображений из файла и их передачи в OCR-модель. 
- Использование более мощных моделей для повышения качества обработки и ослабления ограничений по размеру и языкам. В текущей реализации особенно заметно снижение качества после OCR. В качестве основного текстового инструмента можно рассмотреть GPT или Deepseek, а для OCR — Mistral OCR, LlamaParse.
- Добавить разбиение текста на фрагменты перед отправкой в LLM, чтобы обрабатывать длинные документы частями. Это позволит снизить нагрузку на модель и обработать текст полностью (сейчас берутся 
- В текущем решении таблицы преобразуются в линейный текст или извлекаются через OCR, из-за чего теряется их структура. Для базовой суммаризации это некритично, но при работе с документами, где важны числовые данные и сравнения, качество результата снижается. Поэтому улучшение парсинга таблиц имеет смысл для повышения точности интерпретации содержания.
- В MD-файлах могут присутствовать изображения, вставленные через ссылки — их также можно извлекать и обрабатывать, хотя в большинстве случаев это не должно быть критично.
- Реализовать собственный парсер DOCX внутри Docker-контейнера для снижения зависимости от сторонних сервисов. 
- Добавить в workflow остановку при превышении лимита размера PDF-файла для OCR и вывод сообщений. 
- Добавить боту больше гибкости. Например предоставить пользователю возможности: выбор диапозона страниц для обработки; исключение страниц для обработки; настройка размера суммаризации; выбор языка вывода; добавление дополнительных данных для контекста (словарь, предыдущие документы).
- Добавить функцию поиска похожих документов в базе.
